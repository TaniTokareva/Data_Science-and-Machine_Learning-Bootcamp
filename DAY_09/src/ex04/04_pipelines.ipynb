{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 09. Exercise 04\n",
    "# Pipelines and OOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import svm, tree, ensemble  \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three custom transformers, the first two out of which will be used within a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "1. `FeatureExtractor()` class:\n",
    " - Takes a dataframe with `uid`, `labname`, `numTrials`, `timestamp` from the file [`checker_submits.csv`](https://drive.google.com/file/d/14voc4fNJZiLEFaZyd8nEG-lQt5JjatYw/view?usp=sharing).\n",
    " - Extracts `hour` from `timestamp`.\n",
    " - Extracts `weekday` from `timestamp` (numbers).\n",
    " - Drops the `timestamp` column.\n",
    " - Returns the new dataframe.\n",
    "\n",
    "\n",
    "2. `MyOneHotEncoder()` class:\n",
    " - Takes the dataframe from the result of the previous transformation and the name of the target column.\n",
    " - Identifies all the categorical features and transforms them with `OneHotEncoder()`. If the target column is categorical too, then the transformation should not apply to it.\n",
    " - Drops the initial categorical features.\n",
    " - Returns the dataframe with the features and the series with the target column.\n",
    "\n",
    "\n",
    "3. `TrainValidationTest()` class:\n",
    " - Takes `X` and `y`.\n",
    " - Returns `X_train`, `X_valid`, `X_test`, `y_train`, `y_valid`, `y_test` (`test_size=0.2`, `random_state=21`, `stratified`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline** — это механизм из библиотеки scikit-learn, который позволяет последовательно применять несколько шагов обработки данных, таких как трансформация признаков или обучение модели. Это упрощает процесс обработки данных и делает код более компактным и читаемым. Каждый шаг в Pipeline реализует интерфейсы fit и transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X['hour'] = pd.to_datetime(X['timestamp']).dt.hour\n",
    "        X['dayofweek'] = pd.to_datetime(X['timestamp']).dt.weekday\n",
    "        X = X.drop(columns=['timestamp'])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOneHotEncoder():\n",
    "    def __init__(self, target_column):\n",
    "        self.target_column = target_column\n",
    "        self.encoder = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        categorical_cols = X.select_dtypes(include='object').columns\n",
    "        categorical_cols = [col for col in categorical_cols if col != self.target_column]\n",
    "        # Исключает колонку с целевыми значениями из списка категориальных признаков.\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, drop='first').fit(X[categorical_cols]) \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        categorical_cols = X.select_dtypes(include='object').columns\n",
    "        categorical_cols = [col for col in categorical_cols if col != self.target_column]\n",
    "        \n",
    "        encoded_features = self.encoder.transform(X[categorical_cols])\n",
    "        encoded_df = pd.DataFrame(encoded_features, columns=self.encoder.get_feature_names_out(categorical_cols))\n",
    "        \n",
    "        X = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValidationTest:\n",
    "    def __init__(self, test_size=0.2, random_state=21):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split(self, X, y):\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=self.random_state, stratify=y_temp)\n",
    "        return X_train, X_valid, X_test, y_train, y_valid, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model selection pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelSelection()` class\n",
    "\n",
    " - Takes a list of `GridSearchCV` instances and a dict where the keys are the indexes from that list and the values are the names of the models, the example is below in the reverse order (from high-level to low-level perspective):\n",
    "\n",
    "```\n",
    "ModelSelection(grids, grid_dict)\n",
    "\n",
    "grids = [gs_svm, gs_tree, gs_rf]\n",
    "\n",
    "gs_svm = GridSearchCV(estimator=svm, param_grid=svm_params, scoring='accuracy', cv=2, n_jobs=jobs), where jobs you can specify by yourself\n",
    "\n",
    "svm_params = [{'kernel':('linear', 'rbf', 'sigmoid'), 'C':[0.01, 0.1, 1, 1.5, 5, 10], 'gamma': ['scale', 'auto'], 'class_weight':('balanced', None), 'random_state':[21], 'probability':[True]}]\n",
    "```\n",
    "\n",
    " - Method `choose()` takes `X_train`, `y_train`, `X_valid`, `y_valid` and returns the name of the best classifier among all the models on the validation set\n",
    " - Method `best_results()` returns a dataframe with the columns `model`, `params`, `valid_score` where the rows are the best models within each class of models.\n",
    "\n",
    "```\n",
    "model\tparams\tvalid_score\n",
    "0\tSVM\t{'C': 10, 'class_weight': None, 'gamma': 'auto...\t0.772727\n",
    "1\tDecision Tree\t{'class_weight': 'balanced', 'criterion': 'gin...\t0.801484\n",
    "2\tRandom Forest\t{'class_weight': None, 'criterion': 'entropy',...\t0.855288\n",
    "```\n",
    "\n",
    " - When you iterate through the parameters of a model class, print the name of that class and show the progress using `tqdm.notebook`, in the end of the cycle print the best model of that class.\n",
    "\n",
    "```\n",
    "Estimator: SVM\n",
    "100%\n",
    "125/125 [01:32<00:00, 1.36it/s]\n",
    "Best params: {'C': 10, 'class_weight': None, 'gamma': 'auto', 'kernel': 'rbf', 'probability': True, 'random_state': 21}\n",
    "Best training accuracy: 0.773\n",
    "Validation set accuracy score for best params: 0.878 \n",
    "\n",
    "Estimator: Decision Tree\n",
    "100%\n",
    "57/57 [01:07<00:00, 1.22it/s]\n",
    "Best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 21, 'random_state': 21}\n",
    "Best training accuracy: 0.801\n",
    "Validation set accuracy score for best params: 0.867 \n",
    "\n",
    "Estimator: Random Forest\n",
    "100%\n",
    "284/284 [06:47<00:00, 1.13s/it]\n",
    "Best params: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 22, 'n_estimators': 50, 'random_state': 21}\n",
    "Best training accuracy: 0.855\n",
    "Validation set accuracy score for best params: 0.907 \n",
    "\n",
    "Classifier with best validation set accuracy: Random Forest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция enumerate() в Python — это эффективный инструмент для циклов, создающий пары, состоящие из счётчика и элементов итерируемого объекта. Эти пары упакованы в кортежи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**grids:**\n",
    "Список экземпляров GridSearchCV (один на каждую модель).\n",
    "\n",
    "Каждый экземпляр отвечает за поиск лучших гиперпараметров для конкретной модели.\n",
    "**grid_dict:**\n",
    "Словарь, связывающий индекс модели из списка grids с её именем, например: {0: 'SVM', 1: 'Decision Tree', 2: 'Random Forest'}.\n",
    "\n",
    "\n",
    "**best_model_name:** Хранит название модели с лучшими результатами.\n",
    "\n",
    "**best_model_score:** Начальное значение задаётся минимально возможным.\n",
    "\n",
    "**enumerate(self.grids):** Итерация по всем объектам GridSearchCV с их индексами.\n",
    "\n",
    "**tqdm:** Добавляет индикатор прогресса для удобства наблюдения за выполнением.\n",
    "\n",
    "**self.grid_dict[idx]:** Извлекает имя модели по текущему индексу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection:\n",
    "    def __init__(self, grids, grid_dict):\n",
    "        self.grids = grids\n",
    "        self.grid_dict = grid_dict\n",
    "        self.best_results_df = pd.DataFrame(columns=['model', 'params', 'valid_score'])\n",
    "\n",
    "    \n",
    "    def choose(self, X_train, y_train, X_valid, y_valid):\n",
    "        for idx, grid in enumerate(self.grids):\n",
    "            print(f\"Estimator: {self.grid_dict[idx]}\")\n",
    "            grid.fit(X_train, y_train)\n",
    "            best_params = grid.best_params_\n",
    "            best_score = grid.best_score_\n",
    "            valid_score = grid.score(X_valid, y_valid)\n",
    "\n",
    "            temp_df = pd.DataFrame({\n",
    "                'model': [self.grid_dict[idx]],\n",
    "                'params': [best_params],\n",
    "                'valid_score': [valid_score]\n",
    "            })\n",
    "\n",
    "            self.best_results_df = pd.concat([self.best_results_df, temp_df], ignore_index=True)\n",
    "            \n",
    "            print(f\"Best params: {best_params}\")\n",
    "            print(f\"Validation set accuracy score for best params: {valid_score:.3f}\")\n",
    "        return self.best_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Finalize()` class\n",
    " - Takes an estimator.\n",
    " - Method `final_score()` takes `X_train`, `y_train`, `X_test`, `y_test` and returns the accuracy of the model as in the example below:\n",
    "```\n",
    "final.final_score(X_train, y_train, X_test, y_test)\n",
    "Accuracy of the final model is 0.908284023668639\n",
    "```\n",
    " - Method `save_model()` takes a path, saves the model to this path and prints that the model was successfully saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finalize:\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def final_score(self, X_train, y_train, X_test, y_test):\n",
    "        self.estimator.fit(X_train, y_train)\n",
    "        test_score = self.estimator.score(X_test, y_test)\n",
    "        print(f\"Accuracy of the final model is {test_score:.5f}\")\n",
    "        return test_score\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        joblib.dump(self.estimator, path)\n",
    "        print(f\"Model successfully saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data from the file (****name of file****).\n",
    "2. Create the preprocessing pipeline that consists of two custom transformers: `FeatureExtractor()` and `MyOneHotEncoder()`:\n",
    "```\n",
    "preprocessing = Pipeline([('feature_extractor', FeatureExtractor()), ('onehot_encoder', MyOneHotEncoder('dayofweek'))])\n",
    "```\n",
    "3. Use that pipeline and its method `fit_transform()` on the initial dataset.\n",
    "```\n",
    "data = preprocessing.fit_transform(df)\n",
    "```\n",
    "4. Get `X_train`, `X_valid`, `X_test`, `y_train`, `y_valid`, `y_test` using `TrainValidationTest()` and the result of the pipeline.\n",
    "5. Create an instance of `ModelSelection()`, use the method `choose()` applying it to the models that you want and parameters that you want, get the dataframe of the best results.\n",
    "6. create an instance of `Finalize()` with your best model, use method `final_score()` and save the model in the format: `name_of_the_model_{accuracy on test dataset}.sav`.\n",
    "\n",
    "That is it, congrats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены. Первые строки:\n",
      "      uid   labname  numTrials                   timestamp\n",
      "0  user_4  project1          1  2020-04-17 05:19:02.744528\n",
      "1  user_4  project1          2  2020-04-17 05:22:45.549397\n",
      "2  user_4  project1          3  2020-04-17 05:34:24.422370\n",
      "3  user_4  project1          4  2020-04-17 05:43:27.773992\n",
      "4  user_4  project1          5  2020-04-17 05:46:32.275104\n",
      "Конвейер предобработки успешно выполнен.\n",
      "Обработанные данные:\n",
      "   numTrials  hour  dayofweek  uid_user_1  uid_user_10  uid_user_11  \\\n",
      "0          1     5          4         0.0          0.0          0.0   \n",
      "1          2     5          4         0.0          0.0          0.0   \n",
      "2          3     5          4         0.0          0.0          0.0   \n",
      "3          4     5          4         0.0          0.0          0.0   \n",
      "4          5     5          4         0.0          0.0          0.0   \n",
      "\n",
      "   uid_user_12  uid_user_13  uid_user_14  uid_user_15  ...  labname_lab02  \\\n",
      "0          0.0          0.0          0.0          0.0  ...            0.0   \n",
      "1          0.0          0.0          0.0          0.0  ...            0.0   \n",
      "2          0.0          0.0          0.0          0.0  ...            0.0   \n",
      "3          0.0          0.0          0.0          0.0  ...            0.0   \n",
      "4          0.0          0.0          0.0          0.0  ...            0.0   \n",
      "\n",
      "   labname_lab03  labname_lab03s  labname_lab05s  labname_laba04  \\\n",
      "0            0.0             0.0             0.0             0.0   \n",
      "1            0.0             0.0             0.0             0.0   \n",
      "2            0.0             0.0             0.0             0.0   \n",
      "3            0.0             0.0             0.0             0.0   \n",
      "4            0.0             0.0             0.0             0.0   \n",
      "\n",
      "   labname_laba04s  labname_laba05  labname_laba06  labname_laba06s  \\\n",
      "0              0.0             0.0             0.0              0.0   \n",
      "1              0.0             0.0             0.0              0.0   \n",
      "2              0.0             0.0             0.0              0.0   \n",
      "3              0.0             0.0             0.0              0.0   \n",
      "4              0.0             0.0             0.0              0.0   \n",
      "\n",
      "   labname_project1  \n",
      "0               1.0  \n",
      "1               1.0  \n",
      "2               1.0  \n",
      "3               1.0  \n",
      "4               1.0  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Данные успешно разделены на тренировочные, валидационные и тестовые наборы.\n",
      "Размеры: X_train: (1348, 41), X_valid: (169, 41), X_test: (169, 41)\n",
      "Estimator: SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43767/2165671367.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.best_results_df = pd.concat([self.best_results_df, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 10, 'class_weight': None, 'gamma': 'auto', 'kernel': 'rbf', 'probability': True, 'random_state': 21}\n",
      "Validation set accuracy score for best params: 0.870\n",
      "Estimator: Decision Tree\n",
      "Best params: {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'random_state': 21}\n",
      "Validation set accuracy score for best params: 0.828\n",
      "Estimator: Random Forest\n",
      "Best params: {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'n_estimators': 100, 'random_state': 21}\n",
      "Validation set accuracy score for best params: 0.917\n",
      "Результаты подбора моделей:\n",
      "           model                                             params  \\\n",
      "0            SVM  {'C': 10, 'class_weight': None, 'gamma': 'auto...   \n",
      "1  Decision Tree  {'class_weight': None, 'criterion': 'gini', 'm...   \n",
      "2  Random Forest  {'class_weight': None, 'criterion': 'gini', 'm...   \n",
      "\n",
      "   valid_score  \n",
      "0     0.869822  \n",
      "1     0.828402  \n",
      "2     0.917160  \n",
      "Лучшая модель: Random Forest\n",
      "Accuracy of the final model is 0.95266\n",
      "Model successfully saved to Random Forest_0.95266.sav\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('../data/checker_submits.csv') \n",
    "    print(\"Данные успешно загружены. Первые строки:\")\n",
    "    print(df.head())\n",
    "\n",
    "    #Создание и применение конвейера предобработки\n",
    "    # Конвейер включает два пользовательских трансформера: FeatureExtractor и MyOneHotEncoder\n",
    "    preprocessing = Pipeline([\n",
    "        ('feature_extractor', FeatureExtractor()),  # Извлекает час и день недели, убирает timestamp\n",
    "        ('onehot_encoder', MyOneHotEncoder('dayofweek'))  # Кодирует категориальные признаки\n",
    "    ])\n",
    "    \n",
    "    # Применяем конвейер к данным\n",
    "    data = preprocessing.fit_transform(df)\n",
    "    print(\"Конвейер предобработки успешно выполнен.\")\n",
    "    print(\"Обработанные данные:\")\n",
    "    print(data.head())\n",
    "\n",
    "    target_column = 'dayofweek' \n",
    "    X = data.drop(columns=[target_column]) \n",
    "    y = data[target_column] \n",
    "\n",
    "    # Разделение данных на тренировочные, валидационные и тестовые наборы\n",
    "    splitter = TrainValidationTest(test_size=0.2, random_state=21)\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = splitter.split(X, y)\n",
    "    print(\"Данные успешно разделены на тренировочные, валидационные и тестовые наборы.\")\n",
    "    print(f\"Размеры: X_train: {X_train.shape}, X_valid: {X_valid.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "    # Подбор модели\n",
    "    # Настройки GridSearchCV для разных моделей\n",
    "    svm_params = [{\n",
    "        'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "        'C': [0.01, 0.1, 1, 1.5, 5, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'probability': [True],\n",
    "        'random_state': [21]\n",
    "    }]\n",
    "    tree_params = [{\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 5, 10, 15, 20],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'random_state': [21]\n",
    "    }]\n",
    "    rf_params = [{\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [None, 10, 15, 20],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'random_state': [21]\n",
    "    }]\n",
    "\n",
    "    # Создаем экземпляры GridSearchCV\n",
    "    gs_svm = GridSearchCV(estimator=svm.SVC(), param_grid=svm_params, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "    gs_tree = GridSearchCV(estimator=tree.DecisionTreeClassifier(), param_grid=tree_params, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "    gs_rf = GridSearchCV(estimator=ensemble.RandomForestClassifier(), param_grid=rf_params, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "\n",
    "    # Создаем словарь моделей\n",
    "    grids = [gs_svm, gs_tree, gs_rf]\n",
    "    grid_dict = {0: 'SVM', 1: 'Decision Tree', 2: 'Random Forest'}\n",
    "\n",
    "    # Подбор моделей с помощью ModelSelection\n",
    "    model_selection = ModelSelection(grids, grid_dict)\n",
    "    best_results_df = model_selection.choose(X_train, y_train, X_valid, y_valid)\n",
    "    print(\"Результаты подбора моделей:\")\n",
    "    print(best_results_df)\n",
    "\n",
    "    # Выбор лучшей модели\n",
    "    best_model_index = best_results_df['valid_score'].idxmax()\n",
    "    best_model_name = best_results_df.loc[best_model_index, 'model']\n",
    "    print(f\"Лучшая модель: {best_model_name}\")\n",
    "\n",
    "    # Финализация и сохранение модели\n",
    "    best_estimator = grids[best_model_index].best_estimator_\n",
    "    finalizer = Finalize(best_estimator)\n",
    "\n",
    "    # Оценка точности модели на тестовом наборе\n",
    "    accuracy = finalizer.final_score(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Сохранение модели\n",
    "    model_path = f\"{best_model_name}_{accuracy:.5f}.sav\"\n",
    "    finalizer.save_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
